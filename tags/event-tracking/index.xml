<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Robert Winters</title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://robertwinters.nl/tags/event-tracking/</link>
    <language>en-us</language>
    <author>Rob Winters</author>
    <copyright>2015 Rob Winters</copyright>
    <updated>Sun, 29 Mar 2015 00:00:00 UTC</updated>
    
    
    <item>
      <title>Real-time recommenders, part four -  Creating a recommender</title>
      <link>http://robertwinters.nl/2015/03/2015-03-29-building-simple-recommenders/</link>
      <pubDate>Sun, 29 Mar 2015 00:00:00 UTC</pubDate>
      <author>Rob Winters</author>
      <guid>http://robertwinters.nl/2015/03/2015-03-29-building-simple-recommenders/</guid>
      <description>

&lt;h3 id=&#34;introduction:9ec7d062fbf00e14307a23bc18b30821&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Now that we have both an offline and real-time event stream available, we need to calculate a recommender model to determine what to serve users. Fundamentally, most item-based recommenders work to answer the same question: &amp;ldquo;given product 1, what products (2-N) should I show a customer (and how good of a recommendation are they)&amp;rdquo;? There are a number of complex modeling techniques available and off-the-shelf functions in tools like &lt;a href=&#34;http://mahout.apache.org/&#34;&gt;Mahout&lt;/a&gt; and &lt;a href=&#34;http://prediction.io/&#34;&gt;Prediction.IO&lt;/a&gt;, one of the best ways to familiarize oneself with how these models work is to calculate it yourself. To that end, an easy starting point is a &lt;a href=&#34;http://en.wikipedia.org/wiki/Slope_One&#34;&gt;slope one mode&lt;/a&gt;, a non-trivial way to calculate item and user similarities.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Real-time recommenders, part three -  Event tracking (processing events)</title>
      <link>http://robertwinters.nl/2015/03/2015-03-22-event-processing/</link>
      <pubDate>Sun, 22 Mar 2015 00:00:00 UTC</pubDate>
      <author>Rob Winters</author>
      <guid>http://robertwinters.nl/2015/03/2015-03-22-event-processing/</guid>
      <description>

&lt;h3 id=&#34;introduction:67586a2fe6487ff004d9849a06b13c4b&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In order to utilize our event data from Kinesis we have configured two separate feeds: one non-real-time for reporting and archiving purposes and one real time feed for use cases like recommendations and last viewed items. While snowplow has use case specific libraries for both, we built our own utilities for the following reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; Building our own library allows easier loading to multiple database engines and/or unique archive flows&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Using a basic loader allows event rotation into the database once a minute without any loss of information.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Archive Structure:&lt;/strong&gt; The default snowplow libraries do not make any provision for DB loading and archiving to S3; for reliability purposes we wanted both&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;database-loading:67586a2fe6487ff004d9849a06b13c4b&#34;&gt;Database loading&lt;/h3&gt;

&lt;p&gt;In order to process the data for loading into the database we are using a mixture of the default &lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/3-enrich/scala-kinesis-enrich&#34;&gt;Kinesis Enricher&lt;/a&gt;, sed, a small Python script for event validation, and cronolog to write to a file. The exact flow looks something like:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Kinesis enricher stderr and stdout to stdout &lt;code&gt;/var/lib/snowplow_enrich/snowplow-kinesis-enrich-0.2.0 --config /var/lib/snowplow_enrich/snowplow.conf 2&amp;gt;&amp;amp;1&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;sed to remove checkpointing data from the event stream &lt;code&gt;sed -e &#39;/^\[/d&#39; | sed -e &#39;s/\[pool.*$//&#39;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A Python script which validates whether or not the event has been processed before and validates the custom JSON objects inside the structured event labels&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cronolog which rotates log files every minute &lt;code&gt;cronolog /data/logs/snowplow%Y%m%d%H%M.dat&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One issue with using cronolog is that it splits exactly on the minute, resulting in one event per minute being cut in half every minute. Concatenation of the logs before loading (for example, every 15 minutes) reduces the impact to one event per load, but this is still unacceptable - we would like 100% of events being written to the database. To correct, our loader uses a mixture of &lt;strong&gt;head&lt;/strong&gt; and &lt;strong&gt;tail&lt;/strong&gt; to grab the end of the last event for loading from the head of the log file currently being written. Once the log files are correctly merged, loading and backups become trivial:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

RIGHTNOW=`date +%Y%m%d%H%M`
MINUTEAGO=`date -d &#39;-1 minute&#39; +%Y%m%d%H%M`
YEAR=`date +%Y`
MONTH=`date +%m`
DAY=`date +%d`

find /data/logs -cmin +1 -type f -exec mv &amp;quot;{}&amp;quot; /data/logs/loading/ \;
ls /data/logs/loading/snowplow*.dat | sort | xargs cat&amp;gt; /data/logs/loading/temp.dat
head -n 1 /data/logs/snowplow$MINUTEAGO.dat&amp;gt;&amp;gt;/data/logs/loading/temp.dat
tail -n +2 /data/logs/loading/temp.dat &amp;gt; /data/logs/loading/snowplow_merged_$RIGHTNOW.dat
rm /data/logs/loading/snowplow2*.dat
rm /data/logs/loading/temp.dat

/opt/vertica/bin/vsql -h my.loadbalancer.path -U $1 -w $2 -c &amp;quot;copy stg.snowplow_events from local &#39;/data/logs/loading/snowplow_merged_$RIGHTNOW.dat&#39; with delimiter E&#39;\t&#39;  rejected data &#39;/data/logs/loading/snowplow_rejected_$RIGHTNOW.dat&#39;&amp;quot;

gzip /data/logs/loading/snowplow_merged_$RIGHTNOW.dat

s3cmd put /data/logs/loading/snowplow_merged_$RIGHTNOW.dat.gz s3://bykdwh/snowplow/$YEAR/$MONTH/$DAY/
rm /data/logs/loading/snowplow_merged_$RIGHTNOW.dat.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Far simpler than the default loaders and extremely performant in our environment. Another advantage to having the log files on a minute (or five minute) basis is that it makes it extremely easy to restore an hour or a day if we need to reload.&lt;/p&gt;

&lt;h3 id=&#34;real-time-processing:67586a2fe6487ff004d9849a06b13c4b&#34;&gt;Real time processing&lt;/h3&gt;

&lt;p&gt;While the process outlined above is fantastic for reporting purposes, a one minute delay on the events is still not fast enough for a &amp;ldquo;real time&amp;rdquo; experience for users. Our objective for users is that the data is analyzed and recommendations updated before their next pageview; this gives us five seconds or less to process the event. To achieve this throughput we are using inline processing in a Python script to read specific data from the event, check records against redis, and execute a number of puts  to redis. While this approach is somewhat cumbersome and lower performance than using a language like Scala, it is still possible to achieve throughput of a few thousand events per second per thread - more than sufficient for most businesses.&lt;/p&gt;

&lt;p&gt;The functional workflow of real-time processing looks something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys
import redis
import json
import fileinput

action_value={
    &#39;action1&#39;:weight,
    &#39;action2&#39;:weight,
    &#39;action3&#39;:weight
}


def unlist(input_list):
    return &#39;\t&#39;.join(map(str,input_list))


def process_event(event):
    # Do some event processing to extract the correct user, event type (based on data in the event label), and SKU
    return (user_id, event_type, product_sku)


def main():
    counter = 0
    pipe = rec_set.pipeline()

    for line in sys.stdin:
        try:
            split_tab = []
            split_tab.append(line.split(&#39;\t&#39;))
            split_tab = [val for sublist in split_tab for val in sublist]
            user_id, event_type, product_sku = process_event(split_tab)

            if product_sku is not None:
                # Do some lookups for recommendations and brand information, then load those recommendations to redis
            else:
                next
        except:
            next


if __name__ == &#39;__main__&#39;:
    main()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Real-time recommenders, part two -  Event tracking (front end, collector, and bus)</title>
      <link>http://robertwinters.nl/2015/03/2015-03-14-snowplow-event-capture/</link>
      <pubDate>Sat, 14 Mar 2015 00:00:00 UTC</pubDate>
      <author>Rob Winters</author>
      <guid>http://robertwinters.nl/2015/03/2015-03-14-snowplow-event-capture/</guid>
      <description>

&lt;h3 id=&#34;introduction:e09c72d4c3eec9feb46220c6919223cc&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In order to generate recommendations for users, the first task is to generate user data. One of the best options available right now is to use &lt;a href=&#34;https://github.com/snowplow/snowplow&#34;&gt;Snowplow&lt;/a&gt;, an open source event tracker built around the AWS stack. Using Snoplow plus &lt;a href=&#34;http://www.google.com/tagmanager/&#34;&gt;Google Tag Manager&lt;/a&gt;, one can get event tracking running in just a few hours.&lt;/p&gt;

&lt;h3 id=&#34;front-end-tracker:e09c72d4c3eec9feb46220c6919223cc&#34;&gt;Front End Tracker&lt;/h3&gt;

&lt;p&gt;Rather rewriting already excellent documentation, simply follow the notes &lt;a href=&#34;https://github.com/snowplow/snowplow/wiki/Integrating-javascript-tags-with-Google-Tag-Manager&#34;&gt;here&lt;/a&gt;. However, a few tips in getting started:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Start small:&lt;/strong&gt; Capture Pageviews only and validate against Google Analytics. This will expose potential tracking issues in your front end immediately which might cause tracking issues (we initially were missing almost half of our events due to ajax refreshes).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Be prepared for lots of data:&lt;/strong&gt; Without having a baseline, it&amp;rsquo;s easy to underestimate how much data will arrive. When we turned on page pings on a very conservative interval (every two minutes), &lt;strong&gt;event volume  doubled&lt;/strong&gt; even though average time on page was well under a minute.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Use the existing event types:&lt;/strong&gt; Snowplow is prebuilt to support a number of different events, if at all possible use those rather than &amp;ldquo;rolling your own&amp;rdquo; to capture similar data.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;event-capture-and-bus:e09c72d4c3eec9feb46220c6919223cc&#34;&gt;Event Capture and Bus&lt;/h3&gt;

&lt;p&gt;In order to get real time recommendations, data must be processed in real time; this rules out the &amp;ldquo;production ready&amp;rdquo; clojure collector and forces one to use the scala streaming collector. The good news is that this appears to be extremely stable; we have been using it in production for several months with no significant issues.&lt;/p&gt;

&lt;p&gt;Configuration of the collector is &lt;a href=&#34;https://github.com/snowplow/snowplow/wiki/Setting-up-the-Scala-stream-Collector&#34;&gt;relatively straightforward&lt;/a&gt;. Since our event volume can swing significantly during the day, we set up an AWS image which auto-starts the collector and configured an Elastic Beanstalk application for this purpose. The threshold used is CPU &amp;gt;65% for launching an instance and &amp;lt;25% for removing an instance as this was the greatest cause of event processing delays and dropped data. One major advantage of the streaming collector is that machines can be turned on and off with virtually no loss of data; using the clojure collectors, one can auto-scale up but must manually scale down to ensure that web logs rotate correctly.&lt;/p&gt;

&lt;p&gt;While it is possible to write the event stream directly to stdout (for easy processing), we write the records to Kinesis as we have two consumer streams on the data set: one for real time recommendations and one for recording the data to the DB and archiving data to S3. One issue with Kinesis is that it does &lt;em&gt;not&lt;/em&gt; autoscale. A quick estimate of event volume to bus utilization is that one needs one shard for every 10k events per minute, depending on how much additional data is written per event.&lt;/p&gt;
</description>
    </item>
    
    
  </channel>
</rss>