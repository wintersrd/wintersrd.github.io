<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Robert Danger Winters</title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://robertwinters.nl/tags/aws/</link>
    <language>en-us</language>
    <author>Rob Winters</author>
    <copyright>2015 Rob Winters</copyright>
    <updated>Tue, 24 Nov 2015 00:00:00 UTC</updated>
    
    
    <item>
      <title>TravelBird AWS Case Study</title>
      <link>http://robertwinters.nl/2015/11/aws-case-study/</link>
      <pubDate>Tue, 24 Nov 2015 00:00:00 UTC</pubDate>
      <author>Rob Winters</author>
      <guid>http://robertwinters.nl/2015/11/aws-case-study/</guid>
      <description>&lt;p&gt;AWS did a really nice write-up on what my team has been working on, published &lt;a href=&#34;https://aws.amazon.com/solutions/case-studies/travelbird/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Real-time recommenders, part four -  Creating a recommender</title>
      <link>http://robertwinters.nl/2015/03/2015-03-29-building-simple-recommenders/</link>
      <pubDate>Sun, 29 Mar 2015 00:00:00 UTC</pubDate>
      <author>Rob Winters</author>
      <guid>http://robertwinters.nl/2015/03/2015-03-29-building-simple-recommenders/</guid>
      <description>

&lt;h3 id=&#34;introduction:9ec7d062fbf00e14307a23bc18b30821&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Now that we have both an offline and real-time event stream available, we need to calculate a recommender model to determine what to serve users. Fundamentally, most item-based recommenders work to answer the same question: &amp;ldquo;given product 1, what products (2-N) should I show a customer (and how good of a recommendation are they)&amp;rdquo;? There are a number of complex modeling techniques available and off-the-shelf functions in tools like &lt;a href=&#34;http://mahout.apache.org/&#34;&gt;Mahout&lt;/a&gt; and &lt;a href=&#34;http://prediction.io/&#34;&gt;Prediction.IO&lt;/a&gt;, one of the best ways to familiarize oneself with how these models work is to calculate it yourself. To that end, an easy starting point is a &lt;a href=&#34;http://en.wikipedia.org/wiki/Slope_One&#34;&gt;slope one mode&lt;/a&gt;, a non-trivial way to calculate item and user similarities.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Real-time recommenders, part three -  Event tracking (processing events)</title>
      <link>http://robertwinters.nl/2015/03/2015-03-22-event-processing/</link>
      <pubDate>Sun, 22 Mar 2015 00:00:00 UTC</pubDate>
      <author>Rob Winters</author>
      <guid>http://robertwinters.nl/2015/03/2015-03-22-event-processing/</guid>
      <description>

&lt;h3 id=&#34;introduction:67586a2fe6487ff004d9849a06b13c4b&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In order to utilize our event data from Kinesis we have configured two separate feeds: one non-real-time for reporting and archiving purposes and one real time feed for use cases like recommendations and last viewed items. While snowplow has use case specific libraries for both, we built our own utilities for the following reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; Building our own library allows easier loading to multiple database engines and/or unique archive flows&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Using a basic loader allows event rotation into the database once a minute without any loss of information.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Archive Structure:&lt;/strong&gt; The default snowplow libraries do not make any provision for DB loading and archiving to S3; for reliability purposes we wanted both&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;database-loading:67586a2fe6487ff004d9849a06b13c4b&#34;&gt;Database loading&lt;/h3&gt;

&lt;p&gt;In order to process the data for loading into the database we are using a mixture of the default &lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/3-enrich/scala-kinesis-enrich&#34;&gt;Kinesis Enricher&lt;/a&gt;, sed, a small Python script for event validation, and cronolog to write to a file. The exact flow looks something like:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Kinesis enricher stderr and stdout to stdout &lt;code&gt;/var/lib/snowplow_enrich/snowplow-kinesis-enrich-0.2.0 --config /var/lib/snowplow_enrich/snowplow.conf 2&amp;gt;&amp;amp;1&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;sed to remove checkpointing data from the event stream &lt;code&gt;sed -e &#39;/^\[/d&#39; | sed -e &#39;s/\[pool.*$//&#39;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A Python script which validates whether or not the event has been processed before and validates the custom JSON objects inside the structured event labels&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cronolog which rotates log files every minute &lt;code&gt;cronolog /data/logs/snowplow%Y%m%d%H%M.dat&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One issue with using cronolog is that it splits exactly on the minute, resulting in one event per minute being cut in half every minute. Concatenation of the logs before loading (for example, every 15 minutes) reduces the impact to one event per load, but this is still unacceptable - we would like 100% of events being written to the database. To correct, our loader uses a mixture of &lt;strong&gt;head&lt;/strong&gt; and &lt;strong&gt;tail&lt;/strong&gt; to grab the end of the last event for loading from the head of the log file currently being written. Once the log files are correctly merged, loading and backups become trivial:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

RIGHTNOW=`date +%Y%m%d%H%M`
MINUTEAGO=`date -d &#39;-1 minute&#39; +%Y%m%d%H%M`
YEAR=`date +%Y`
MONTH=`date +%m`
DAY=`date +%d`

find /data/logs -cmin +1 -type f -exec mv &amp;quot;{}&amp;quot; /data/logs/loading/ \;
ls /data/logs/loading/snowplow*.dat | sort | xargs cat&amp;gt; /data/logs/loading/temp.dat
head -n 1 /data/logs/snowplow$MINUTEAGO.dat&amp;gt;&amp;gt;/data/logs/loading/temp.dat
tail -n +2 /data/logs/loading/temp.dat &amp;gt; /data/logs/loading/snowplow_merged_$RIGHTNOW.dat
rm /data/logs/loading/snowplow2*.dat
rm /data/logs/loading/temp.dat

/opt/vertica/bin/vsql -h my.loadbalancer.path -U $1 -w $2 -c &amp;quot;copy stg.snowplow_events from local &#39;/data/logs/loading/snowplow_merged_$RIGHTNOW.dat&#39; with delimiter E&#39;\t&#39;  rejected data &#39;/data/logs/loading/snowplow_rejected_$RIGHTNOW.dat&#39;&amp;quot;

gzip /data/logs/loading/snowplow_merged_$RIGHTNOW.dat

s3cmd put /data/logs/loading/snowplow_merged_$RIGHTNOW.dat.gz s3://bykdwh/snowplow/$YEAR/$MONTH/$DAY/
rm /data/logs/loading/snowplow_merged_$RIGHTNOW.dat.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Far simpler than the default loaders and extremely performant in our environment. Another advantage to having the log files on a minute (or five minute) basis is that it makes it extremely easy to restore an hour or a day if we need to reload.&lt;/p&gt;

&lt;h3 id=&#34;real-time-processing:67586a2fe6487ff004d9849a06b13c4b&#34;&gt;Real time processing&lt;/h3&gt;

&lt;p&gt;While the process outlined above is fantastic for reporting purposes, a one minute delay on the events is still not fast enough for a &amp;ldquo;real time&amp;rdquo; experience for users. Our objective for users is that the data is analyzed and recommendations updated before their next pageview; this gives us five seconds or less to process the event. To achieve this throughput we are using inline processing in a Python script to read specific data from the event, check records against redis, and execute a number of puts  to redis. While this approach is somewhat cumbersome and lower performance than using a language like Scala, it is still possible to achieve throughput of a few thousand events per second per thread - more than sufficient for most businesses.&lt;/p&gt;

&lt;p&gt;The functional workflow of real-time processing looks something like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys
import redis
import json
import fileinput

action_value={
    &#39;action1&#39;:weight,
    &#39;action2&#39;:weight,
    &#39;action3&#39;:weight
}


def unlist(input_list):
    return &#39;\t&#39;.join(map(str,input_list))


def process_event(event):
    # Do some event processing to extract the correct user, event type (based on data in the event label), and SKU
    return (user_id, event_type, product_sku)


def main():
    counter = 0
    pipe = rec_set.pipeline()

    for line in sys.stdin:
        try:
            split_tab = []
            split_tab.append(line.split(&#39;\t&#39;))
            split_tab = [val for sublist in split_tab for val in sublist]
            user_id, event_type, product_sku = process_event(split_tab)

            if product_sku is not None:
                # Do some lookups for recommendations and brand information, then load those recommendations to redis
            else:
                next
        except:
            next


if __name__ == &#39;__main__&#39;:
    main()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Real-time recommenders, part two -  Event tracking (front end, collector, and bus)</title>
      <link>http://robertwinters.nl/2015/03/2015-03-14-snowplow-event-capture/</link>
      <pubDate>Sat, 14 Mar 2015 00:00:00 UTC</pubDate>
      <author>Rob Winters</author>
      <guid>http://robertwinters.nl/2015/03/2015-03-14-snowplow-event-capture/</guid>
      <description>

&lt;h3 id=&#34;introduction:e09c72d4c3eec9feb46220c6919223cc&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In order to generate recommendations for users, the first task is to generate user data. One of the best options available right now is to use &lt;a href=&#34;https://github.com/snowplow/snowplow&#34;&gt;Snowplow&lt;/a&gt;, an open source event tracker built around the AWS stack. Using Snoplow plus &lt;a href=&#34;http://www.google.com/tagmanager/&#34;&gt;Google Tag Manager&lt;/a&gt;, one can get event tracking running in just a few hours.&lt;/p&gt;

&lt;h3 id=&#34;front-end-tracker:e09c72d4c3eec9feb46220c6919223cc&#34;&gt;Front End Tracker&lt;/h3&gt;

&lt;p&gt;Rather rewriting already excellent documentation, simply follow the notes &lt;a href=&#34;https://github.com/snowplow/snowplow/wiki/Integrating-javascript-tags-with-Google-Tag-Manager&#34;&gt;here&lt;/a&gt;. However, a few tips in getting started:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Start small:&lt;/strong&gt; Capture Pageviews only and validate against Google Analytics. This will expose potential tracking issues in your front end immediately which might cause tracking issues (we initially were missing almost half of our events due to ajax refreshes).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Be prepared for lots of data:&lt;/strong&gt; Without having a baseline, it&amp;rsquo;s easy to underestimate how much data will arrive. When we turned on page pings on a very conservative interval (every two minutes), &lt;strong&gt;event volume  doubled&lt;/strong&gt; even though average time on page was well under a minute.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Use the existing event types:&lt;/strong&gt; Snowplow is prebuilt to support a number of different events, if at all possible use those rather than &amp;ldquo;rolling your own&amp;rdquo; to capture similar data.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;event-capture-and-bus:e09c72d4c3eec9feb46220c6919223cc&#34;&gt;Event Capture and Bus&lt;/h3&gt;

&lt;p&gt;In order to get real time recommendations, data must be processed in real time; this rules out the &amp;ldquo;production ready&amp;rdquo; clojure collector and forces one to use the scala streaming collector. The good news is that this appears to be extremely stable; we have been using it in production for several months with no significant issues.&lt;/p&gt;

&lt;p&gt;Configuration of the collector is &lt;a href=&#34;https://github.com/snowplow/snowplow/wiki/Setting-up-the-Scala-stream-Collector&#34;&gt;relatively straightforward&lt;/a&gt;. Since our event volume can swing significantly during the day, we set up an AWS image which auto-starts the collector and configured an Elastic Beanstalk application for this purpose. The threshold used is CPU &amp;gt;65% for launching an instance and &amp;lt;25% for removing an instance as this was the greatest cause of event processing delays and dropped data. One major advantage of the streaming collector is that machines can be turned on and off with virtually no loss of data; using the clojure collectors, one can auto-scale up but must manually scale down to ensure that web logs rotate correctly.&lt;/p&gt;

&lt;p&gt;While it is possible to write the event stream directly to stdout (for easy processing), we write the records to Kinesis as we have two consumer streams on the data set: one for real time recommendations and one for recording the data to the DB and archiving data to S3. One issue with Kinesis is that it does &lt;em&gt;not&lt;/em&gt; autoscale. A quick estimate of event volume to bus utilization is that one needs one shard for every 10k events per minute, depending on how much additional data is written per event.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>Automating Tableau Backups via AWS</title>
      <link>http://robertwinters.nl/2015/03/2015-03-08-automatic-tableau-backups-to-s3/</link>
      <pubDate>Sun, 08 Mar 2015 00:00:00 UTC</pubDate>
      <author>Rob Winters</author>
      <guid>http://robertwinters.nl/2015/03/2015-03-08-automatic-tableau-backups-to-s3/</guid>
      <description>

&lt;h3 id=&#34;introduction:6a2ccbe037512dd4d211ba76c4c998d1&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Tableau is far and away my favorite Business Intelligence tool for its ease of development and performance on small (&amp;lt;20M row) extracts, but I have always found the backup approach disappointing. Each backup can account for a substantial amount of disk space, there is no default backup rotation, and Windows offers insufficient tooling to effectively handle these tasks. Enter &lt;a href=&#34;https://boto.readthedocs.org/en/latest/&#34;&gt;boto&lt;/a&gt;, the Python library for AWS. Using boto plus S3 and tabadmin, we were able to build a backup solution for Tableau which is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Infinitely space scalable&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Fully automated and can email on failure&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Handles daily/weekly/monthly backup rotation automatically. Currently we keep daily for two weeks, weekly for six months, and monthly for an indefinite period.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Low cost. As of February 2015 the storage cost of S3 is approximately $0.03 per month, so keeping backups only runs a few tens of dollars per year.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;components:6a2ccbe037512dd4d211ba76c4c998d1&#34;&gt;Components&lt;/h3&gt;

&lt;p&gt;There are several critical functions to generating and rotating the backups effectively:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Options manager:&lt;/strong&gt; To read configuration details like AWS keys, bucket names, file naming conventions, etc&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Backup extractor:&lt;/strong&gt; To pull the backup, in this case just a system call to tabadmin&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Uploader:&lt;/strong&gt; Used to place the file in S3 and validate its placement. Given the file sizes, this must be a multipart upload&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Backup Rotator:&lt;/strong&gt; Used to eliminate backups which no longer fit our retention policies&lt;/p&gt;

&lt;h3 id=&#34;options-management:6a2ccbe037512dd4d211ba76c4c998d1&#34;&gt;Options Management&lt;/h3&gt;

&lt;p&gt;Options were handled using the argparse and option parser libraries to store configuration details on our Tableau server; our configuration options were:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[aws]
key: 
secret: 
bucket: 
days_to_keep_daily: 14
days_to_keep_weekly: 180


[Tableau] 
tempdir: C:/BACKUPS/
tabadmin_path: C:/Program Files/Tableau/Tableau Server/8.3/bin/tabadmin.exe
filename_base: TABLEAU_BACKUP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This affords the flexibility to easily change rotation schedule in the future.&lt;/p&gt;

&lt;h3 id=&#34;backup-extraction:6a2ccbe037512dd4d211ba76c4c998d1&#34;&gt;Backup Extraction&lt;/h3&gt;

&lt;p&gt;Handled using a simple system call to the tabadmin command based on the parameters set in the config file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_extract(configs):
	call(configs.get(&#39;Tableau&#39;,&#39;tabadmin_path&#39;) + &amp;quot; backup -d &amp;quot; + configs.get(&#39;Tableau&#39;,&#39;tempdir&#39;) + configs.get(&#39;Tableau&#39;,&#39;filename_base&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;uploading-and-validating:6a2ccbe037512dd4d211ba76c4c998d1&#34;&gt;Uploading and validating&lt;/h3&gt;

&lt;p&gt;To upload the backup and validate the object, we built a number of small functions to upload the file and confirm its placement. Our files are laoded in a naming convention S3://{bucket}/{tableau_path}/{year}/{month}/{filename_root}-YYYY-MM-DD.tsbak which makes it easy to recover the appropriate backup. Specific functions used include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A multipart file uploader which is &amp;ldquo;borrowed&amp;rdquo; wholesale from &lt;a href=&#34;http://boto.readthedocs.org/en/latest/s3_tut.html#storing-large-data&#34;&gt;boto&amp;rsquo;s S3 examples&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A small function to validate the upload was successful using bucket.list()&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;file-rotation:6a2ccbe037512dd4d211ba76c4c998d1&#34;&gt;File rotation&lt;/h3&gt;

&lt;p&gt;Since we are backing up daily but have a tiered backup strategy, the most trivial way to do so is to generate a list of valid file names and remove any which do not conform to the list. Specifically, we will need:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;All file names which fit in the daily retention window (14 days ago through today)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;All file names which match a Sunday (chosen snapshot day) and are between six months ago and today&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;All file names which have &amp;ldquo;01&amp;rdquo; as the date, regardless of year/month&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As a day can fit 0-3 of these conditions, the easiest solution was to generate three distinct lists, merge them, and deduplicate them using the set function. We can then pull a list of all keys from S3 for our Tableau backups, filtering them, and then deleting the undesired files. One caveat associated with S3 keys from boto: the file name is treated is the entire path after the bucket, so it is necessary to parse out the relevant information for finding the correct file. To do so, the function below is used:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def remove_old(connection, configs):
	keys = connection.list(&#39;tableau_backups&#39;)
	valid_keys = keys_to_keep(connection, configs)
	keylist = []
	for key in keys:
		keyname = key.key.rsplit(&#39;/&#39;,1)[1]
		if keyname not in valid_keys:
			keylist.append(key.key)
	for j in keylist:
		connection.delete_key(j)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;conclusion:6a2ccbe037512dd4d211ba76c4c998d1&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In addition to the functionality outlined above, it&amp;rsquo;s also helpful to add in email notification in the event of failure and some additional logging. That said, this utility massively reduces data loss risks with Tableau and introduces a useful, low cost mechanism to maintain any easily restorable history of the server.&lt;/p&gt;
</description>
    </item>
    
    
  </channel>
</rss>